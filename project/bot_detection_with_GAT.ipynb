{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Libs\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import warnings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# Reference for the classes: https://github.com/keras-team/keras-io/blob/master/examples/graph/gat_node_classification.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Some constants we are gonna use\n",
    "DATA_INPUT_PATH = 'data'\n",
    "DATA_INPUT_NAME2 = 'dev.json'\n",
    "DATA_INPUT_NAME3 = 'train.json'\n",
    "DATA_INPUT_NAME4 = 'test.json'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# Openning the files provided by the academic community\n",
    "# https://arxiv.org/abs/2106.13088\n",
    "with open(os.path.join(DATA_INPUT_PATH, DATA_INPUT_NAME2)) as file_json:\n",
    "    data_twitter_dev = json.load(file_json)\n",
    "with open(os.path.join(DATA_INPUT_PATH, DATA_INPUT_NAME3)) as file_json:\n",
    "    data_twitter_train = json.load(file_json)\n",
    "with open(os.path.join(DATA_INPUT_PATH, DATA_INPUT_NAME4)) as file_json:\n",
    "    data_twitter_test = json.load(file_json)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# Creating the dataset from json records and fields\n",
    "df_data_twitter_all = pd.DataFrame.from_records(data_twitter_train).append(pd.DataFrame.from_records(data_twitter_dev)).append(pd.DataFrame.from_records(data_twitter_test))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Function to retrieve followings of each user\n",
    "def retrieve_following(x):\n",
    "    try:\n",
    "        f = x['following']\n",
    "        if f == []:\n",
    "            f = [-999]\n",
    "        else:\n",
    "            f = f\n",
    "    except:\n",
    "        f = [-999]\n",
    "    return f"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Getting the profile features for each user and the label \n",
    "df_data_twitter_all_features = pd.DataFrame.from_dict(df_data_twitter_all['profile'].tolist())\n",
    "df_data_twitter_all_features['label'] =  df_data_twitter_all['label'].reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "df_data_twitter_all_features.to_csv('./data/user_features_all.csv')\n",
    "# Retrieving neigbors for each user\n",
    "df_neighbors_twitter_all = pd.DataFrame.from_dict(df_data_twitter_all['neighbor'])\n",
    "df_neighbors_twitter_all['ID'] = pd.DataFrame.from_dict(df_data_twitter_all['ID'])\n",
    "df_neighbors_twitter_all.dropna(inplace=True)\n",
    "df_neighbors_twitter_all['neighbor'] = df_neighbors_twitter_all['neighbor'].apply(lambda x: np.array(x['following']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Creating a dataframe in wich each row has 2 columns: Source and target. Source is and ID of an user and Target \n",
    "# Is the ID of other user which is connected with the source (source follows target)\n",
    "array_source_target = np.empty((1,2))\n",
    "for _, user in  df_neighbors_twitter_all.iterrows():\n",
    "    following = user['neighbor']\n",
    "    user_ = np.array([user['ID']])\n",
    "    array_user_following = np.hstack([np.broadcast_to(user_, shape=(len(following), len(user_))), following.reshape(-1,1)])\n",
    "    #array_user_follower = np.hstack([np.broadcast_to(user_, shape=(len(followers), len(user_))), followers.reshape(-1,1)])\n",
    "    array_source_target = np.vstack([array_source_target,array_user_following])\n",
    "df_source_target_all = pd.DataFrame(array_source_target, columns=['source', 'target'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "df_source_target_all.drop(0, axis=0, inplace=True)\n",
    "df_source_target_all.to_csv('./data/source_target_all.csv')\n",
    "# Data prep - False:0 and True: 1\n",
    "df_data_twitter_all_features = df_data_twitter_all_features.replace({'False ':0, 'True ':1})\n",
    "df_source_target_all = df_source_target_all.astype('int')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# listing the features we are gonna use\n",
    "list_features_to_convert = ['id', 'protected', 'followers_count', 'friends_count', 'listed_count', 'favourites_count', 'statuses_count', 'label']\n",
    "df_data_twitter_all_features[list_features_to_convert] = df_data_twitter_all_features[list_features_to_convert].astype('int')\n",
    "# Casting all to int\n",
    "df_data_twitter_all_features_bool = df_data_twitter_all_features.select_dtypes(include=['int'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# clases\n",
    "class_values = [0, 1]\n",
    "# id para as classes\n",
    "class_idx = {name: id for id, name in enumerate(class_values)}\n",
    "# id para os papers\n",
    "user_idx = {int(name): int(idx) for idx, name in enumerate(sorted(df_data_twitter_all_features_bool['id'].tolist()))}\n",
    "df_data_twitter_all_features_bool['id'] = df_data_twitter_all_features_bool['id'].apply(lambda x: user_idx.get(x, np.nan))\n",
    "df_source_target_all['target'] = df_source_target_all['target'].apply(lambda x: user_idx.get(x, np.nan))\n",
    "df_source_target_all['source'] = df_source_target_all['source'].apply(lambda x: user_idx.get(x, np.nan))\n",
    "df_data_twitter_all_features_bool['label'] = df_data_twitter_all_features_bool['label'].apply(lambda x: class_idx.get(x, np.nan))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/5d/5_bh9sv53fl6r16rkjts69k80000gn/T/ipykernel_81475/1076936382.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_data_twitter_all_features_bool['id'] = df_data_twitter_all_features_bool['id'].apply(lambda x: user_idx.get(x, np.nan))\n",
      "/var/folders/5d/5_bh9sv53fl6r16rkjts69k80000gn/T/ipykernel_81475/1076936382.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_data_twitter_all_features_bool['label'] = df_data_twitter_all_features_bool['label'].apply(lambda x: class_idx.get(x, np.nan))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "df_source_target_all.dropna(inplace=True)\n",
    "df_source_target_all = df_source_target_all.astype('int')\n",
    "df_source_target_all.reset_index(drop=True, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "df_data_twitter_all_features_bool['Label'] = df_data_twitter_all_features_bool['id']\n",
    "df_data_twitter_all_features_bool['Id'] = df_data_twitter_all_features_bool['id']\n",
    "df_data_twitter_all_features_bool[['Id', 'Label']].to_csv('./data/id_label.csv')\n",
    "df_source_target_all['Type'] = 'Directed'\n",
    "df_source_target_all['Source'] = df_source_target_all['source']\n",
    "df_source_target_all['Target'] = df_source_target_all['target']\n",
    "df_source_target_all[['Source', 'Target', 'Type']].to_csv('./data/source_target_ids_all.csv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/5d/5_bh9sv53fl6r16rkjts69k80000gn/T/ipykernel_81475/1428570426.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_data_twitter_all_features_bool['Label'] = df_data_twitter_all_features_bool['id']\n",
      "/var/folders/5d/5_bh9sv53fl6r16rkjts69k80000gn/T/ipykernel_81475/1428570426.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_data_twitter_all_features_bool['Id'] = df_data_twitter_all_features_bool['id']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# Obtain random indices\n",
    "np.random.seed(100)\n",
    "random_indices = np.random.permutation(range(df_data_twitter_all_features_bool.shape[0]))\n",
    "n = int(len(random_indices)*0.8)\n",
    "# 50/50 split\n",
    "train_data = df_data_twitter_all_features_bool.iloc[random_indices[:n]]\n",
    "test_data = df_data_twitter_all_features_bool.iloc[random_indices[n:]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# Obtain paper indices which will be used to gather node states\n",
    "# from the graph later on when training the model\n",
    "train_indices = train_data[\"id\"].to_numpy()\n",
    "test_indices = test_data[\"id\"].to_numpy()\n",
    "\n",
    "# Obtain ground truth labels corresponding to each paper_id\n",
    "train_labels = train_data[\"label\"].to_numpy()\n",
    "test_labels = test_data[\"label\"].to_numpy()\n",
    "\n",
    "# Define graph, namely an edge tensor and a node feature tensor\n",
    "edges = tf.convert_to_tensor(df_source_target_all[[\"source\", \"target\"]])\n",
    "node_features = tf.convert_to_tensor(df_data_twitter_all_features_bool.sort_values(\"id\").iloc[:, 1:-1])\n",
    "\n",
    "# Print shapes of the graph\n",
    "print(\"Edges shape:\\t\\t\", edges.shape)\n",
    "print(\"Node features shape:\", node_features.shape)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Edges shape:\t\t (11890, 2)\n",
      "Node features shape: (11826, 18)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-20 09:25:07.303784: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    " class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "        )\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_features, edges = inputs\n",
    "\n",
    "        # Linearly transform node features (node states)\n",
    "        node_features_transformed = tf.matmul(node_features, self.kernel)\n",
    "\n",
    "        # (1) Compute pair-wise attention scores\n",
    "        node_features_expanded = tf.gather(node_features_transformed, edges)\n",
    "        node_features_expanded = tf.reshape(\n",
    "            node_features_expanded, (tf.shape(edges)[0], -1)\n",
    "        )\n",
    "        attention_scores = tf.nn.leaky_relu(\n",
    "            tf.matmul(node_features_expanded, self.kernel_attention)\n",
    "        )\n",
    "        attention_scores = tf.squeeze(attention_scores, -1)\n",
    "\n",
    "        # (2) Normalize attention scores\n",
    "        attention_scores = tf.math.exp(tf.clip_by_value(attention_scores, -2, 2))\n",
    "        attention_scores_sum = tf.math.unsorted_segment_sum(\n",
    "            data=attention_scores,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.reduce_max(edges[:, 0]) + 1,\n",
    "        )\n",
    "        attention_scores_sum = tf.repeat(\n",
    "            attention_scores_sum, tf.math.bincount(tf.cast(edges[:, 0], \"int32\"))\n",
    "        )\n",
    "        attention_scores_norm = attention_scores / attention_scores_sum\n",
    "\n",
    "        # (3) Gather node states of neighbors, apply attention scores and aggregate\n",
    "        node_features_neighbors = tf.gather(node_features_transformed, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=node_features_neighbors * attention_scores_norm[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_features)[0],\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_features, pair_indices = inputs\n",
    "\n",
    "        # Obtain outputs from each attention head\n",
    "        outputs = [\n",
    "            attention_layer([atom_features, pair_indices])\n",
    "            for attention_layer in self.attention_layers\n",
    "        ]\n",
    "        # Concatenate or average the node states from each head\n",
    "        if self.merge_type == \"concat\":\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            outputs = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        # Activate and return node states\n",
    "        return tf.nn.relu(outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "        )\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_features, edges = inputs\n",
    "\n",
    "        # Linearly transform node features (node states)\n",
    "        node_features_transformed = tf.matmul(node_features, self.kernel)\n",
    "\n",
    "        # (1) Compute pair-wise attention scores\n",
    "        node_features_expanded = tf.gather(node_features_transformed, edges)\n",
    "        node_features_expanded = tf.reshape(\n",
    "            node_features_expanded, (tf.shape(edges)[0], -1)\n",
    "        )\n",
    "        attention_scores = tf.nn.leaky_relu(\n",
    "            tf.matmul(node_features_expanded, self.kernel_attention)\n",
    "        )\n",
    "        attention_scores = tf.squeeze(attention_scores, -1)\n",
    "\n",
    "        # (2) Normalize attention scores\n",
    "        attention_scores = tf.math.exp(tf.clip_by_value(attention_scores, -2, 2))\n",
    "        attention_scores_sum = tf.math.unsorted_segment_sum(\n",
    "            data=attention_scores,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.reduce_max(edges[:, 0]) + 1,\n",
    "        )\n",
    "        attention_scores_sum = tf.repeat(\n",
    "            attention_scores_sum, tf.math.bincount(tf.cast(edges[:, 0], \"int32\"))\n",
    "        )\n",
    "        attention_scores_norm = attention_scores / attention_scores_sum\n",
    "\n",
    "        # (3) Gather node states of neighbors, apply attention scores and aggregate\n",
    "        node_features_neighbors = tf.gather(node_features_transformed, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=node_features_neighbors * attention_scores_norm[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_features)[0],\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_features, pair_indices = inputs\n",
    "\n",
    "        # Obtain outputs from each attention head\n",
    "        outputs = [\n",
    "            attention_layer([atom_features, pair_indices])\n",
    "            for attention_layer in self.attention_layers\n",
    "        ]\n",
    "        # Concatenate or average the node states from each head\n",
    "        if self.merge_type == \"concat\":\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            outputs = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        # Activate and return node states\n",
    "        return tf.nn.relu(outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_features,\n",
    "        edges,\n",
    "        hidden_units,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        output_dim,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
    "        self.attention_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_features, edges = inputs\n",
    "        x = self.preprocess(node_features)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer([x, edges]) + x\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        indices, labels = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            outputs = self([self.node_features, self.edges])\n",
    "            # Compute loss\n",
    "            loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        # Apply gradients (update weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        # Update metric(s)\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_features, self.edges])\n",
    "        # Compute probabilities\n",
    "        return tf.nn.softmax(tf.gather(outputs, indices))\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, labels = data\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_features, self.edges])\n",
    "        # Compute loss\n",
    "        loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        # Update metric(s)\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# Define hyper-parameters\n",
    "HIDDEN_UNITS = 100\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "OUTPUT_DIM = len(class_values)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 3e-1\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam(LEARNING_RATE)\n",
    "accuracy_fn = keras.metrics.SparseCategoricalAccuracy(name=\"acc\")\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_acc\", min_delta=1e-5, patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Build model\n",
    "gat_model = GraphAttentionNetwork(\n",
    "    node_features, edges, HIDDEN_UNITS, NUM_HEADS, NUM_LAYERS, OUTPUT_DIM\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "gat_model.compile(loss=loss_fn, optimizer=optimizer, metrics=[accuracy_fn])\n",
    "\n",
    "gat_model.fit(\n",
    "    x=train_indices,\n",
    "    y=train_labels,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "_, test_accuracy = gat_model.evaluate(x=test_indices, y=test_labels, verbose=0)\n",
    "\n",
    "print(\"--\" * 38 + f\"\\nTest Accuracy {test_accuracy*100:.1f}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-20 09:25:07.605670: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "34/34 - 71s - loss: 2932511604736.0000 - acc: 0.4866 - val_loss: 213604597760.0000 - val_acc: 0.4715\n",
      "Epoch 2/100\n",
      "34/34 - 92s - loss: 260003872768.0000 - acc: 0.4725 - val_loss: 899242917888.0000 - val_acc: 0.4746\n",
      "Epoch 3/100\n",
      "34/34 - 72s - loss: 42957647872.0000 - acc: 0.4702 - val_loss: 89943695360.0000 - val_acc: 0.5106\n",
      "Epoch 4/100\n",
      "34/34 - 68s - loss: 237015728128.0000 - acc: 0.4663 - val_loss: 41093193728.0000 - val_acc: 0.5053\n",
      "Epoch 5/100\n",
      "34/34 - 73s - loss: 105348784128.0000 - acc: 0.4733 - val_loss: 104669241344.0000 - val_acc: 0.5233\n",
      "Epoch 6/100\n",
      "34/34 - 59s - loss: 100939177984.0000 - acc: 0.4785 - val_loss: 21179029504.0000 - val_acc: 0.5032\n",
      "Epoch 7/100\n",
      "34/34 - 60s - loss: 18365208576.0000 - acc: 0.4777 - val_loss: 3736955904.0000 - val_acc: 0.5063\n",
      "Epoch 8/100\n",
      "34/34 - 63s - loss: 8580612096.0000 - acc: 0.4792 - val_loss: 7638352384.0000 - val_acc: 0.4810\n",
      "Epoch 9/100\n",
      "34/34 - 72s - loss: 6369082880.0000 - acc: 0.4737 - val_loss: 20396111872.0000 - val_acc: 0.5053\n",
      "Epoch 10/100\n",
      "34/34 - 76s - loss: 2527453440.0000 - acc: 0.4804 - val_loss: 2753443584.0000 - val_acc: 0.4905\n",
      "----------------------------------------------------------------------------\n",
      "Test Accuracy 50.6%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "test_probs = gat_model.predict(x=test_indices)\n",
    "\n",
    "mapping = {v: k for (k, v) in class_idx.items()}\n",
    "\n",
    "for i, (probs, label) in enumerate(zip(test_probs[:2], test_labels[:2])):\n",
    "    print(f\"Example {i+1}: {mapping[label]}\")\n",
    "    for j, c in zip(probs, class_idx.keys()):\n",
    "        print(f\"\\tProbability of {c: <24} = {j*100:7.3f}%\")\n",
    "    print(\"---\" * 20)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Example 1: 0\n",
      "\tProbability of 0                        = 100.000%\n",
      "\tProbability of 1                        =   0.000%\n",
      "------------------------------------------------------------\n",
      "Example 2: 0\n",
      "\tProbability of 0                        =   0.000%\n",
      "\tProbability of 1                        = 100.000%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Ressalvas: Estamos utilizando apenas um conjunto pequeno de features de perfil para validar a \n",
    "# aplicação da GAT na detecção de BOTs. Além disso, não otimizamos hiperparâmetros, estamos usando apenas as relações diretas,o que torna\n",
    "# o problema bastante esparso. \n",
    "# Para próximos passos deveremos incluir, além das features de perfil, os tweets como um embedding semântico\n",
    "# E também a vizinhança nível 2 (amigos dos meus amigos) para tornar o grafo mais denso\n",
    "# nesse primeiro momento, com a arquitetura disponível para utilização e com nossos dados, nosso classificador\n",
    "# Está com um erro altíssimo. "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('mestrado_env': venv)"
  },
  "interpreter": {
   "hash": "e986b0d8e9e3f955479e64620b309f8ea549b3c72c8146b7e60698efb365122b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}